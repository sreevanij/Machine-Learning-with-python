{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import tensorflow.keras.utils as ku\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from fairest creatures we desire increase,\n"
     ]
    }
   ],
   "source": [
    "data = open('poetry.txt').read()\n",
    "corpus = data.lower().split(\"\\n\")\n",
    "\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3211\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "total_words = len(tokenizer.word_index)+1\n",
    "\n",
    "print(total_words)\n",
    "input_sequences =[]\n",
    "\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    #print(token_list)\n",
    "    for i in range(1,len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "#print(input_sequences)\n",
    "    \n",
    "    \n",
    "max_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences,maxlen=max_len,padding='pre'))\n",
    "\n",
    "\n",
    "predictions,label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "\n",
    "label = ku.to_categorical(label,num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15462, 10)\n",
      "(15462, 10)\n"
     ]
    }
   ],
   "source": [
    "print(predictions.shape)\n",
    "#np.expand_dims(predictions,axis=2)\n",
    "#np.expand_dims(label,axis=2)\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(total_words,32,input_length=max_len-1),\n",
    "    tf.keras.layers.Bidirectional(LSTM(32,input_shape=(32,10),return_sequences=True)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Bidirectional(LSTM(32)),\n",
    "    tf.keras.layers.Dense(64,activation='relu'),\n",
    "    tf.keras.layers.Dense(total_words,activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_27 (Embedding)     (None, 10, 32)            102752    \n",
      "_________________________________________________________________\n",
      "bidirectional_42 (Bidirectio (None, 10, 64)            16640     \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 10, 64)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_43 (Bidirectio (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 3211)              208715    \n",
      "=================================================================\n",
      "Total params: 357,099\n",
      "Trainable params: 357,099\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15462 samples\n",
      "Epoch 1/100\n",
      "15462/15462 [==============================] - 11s 715us/sample - loss: 5.4262 - accuracy: 0.0558\n",
      "Epoch 2/100\n",
      "15462/15462 [==============================] - 11s 711us/sample - loss: 5.3176 - accuracy: 0.0627\n",
      "Epoch 3/100\n",
      "15462/15462 [==============================] - 11s 734us/sample - loss: 5.2208 - accuracy: 0.0627\n",
      "Epoch 4/100\n",
      "15462/15462 [==============================] - 11s 731us/sample - loss: 5.1205 - accuracy: 0.0676\n",
      "Epoch 5/100\n",
      "15462/15462 [==============================] - 11s 733us/sample - loss: 5.0131 - accuracy: 0.0726\n",
      "Epoch 6/100\n",
      "15462/15462 [==============================] - 11s 728us/sample - loss: 4.9134 - accuracy: 0.0761\n",
      "Epoch 7/100\n",
      "15462/15462 [==============================] - 11s 727us/sample - loss: 4.8112 - accuracy: 0.0774\n",
      "Epoch 8/100\n",
      "15462/15462 [==============================] - 12s 764us/sample - loss: 4.7066 - accuracy: 0.0854\n",
      "Epoch 9/100\n",
      "15462/15462 [==============================] - 12s 786us/sample - loss: 4.6077 - accuracy: 0.0916\n",
      "Epoch 10/100\n",
      "15462/15462 [==============================] - 11s 734us/sample - loss: 4.5038 - accuracy: 0.0991\n",
      "Epoch 11/100\n",
      "15462/15462 [==============================] - 11s 729us/sample - loss: 4.3999 - accuracy: 0.1057\n",
      "Epoch 12/100\n",
      "15462/15462 [==============================] - 11s 714us/sample - loss: 4.2995 - accuracy: 0.1140\n",
      "Epoch 13/100\n",
      "15462/15462 [==============================] - 12s 790us/sample - loss: 4.2003 - accuracy: 0.1236\n",
      "Epoch 14/100\n",
      "15462/15462 [==============================] - 14s 909us/sample - loss: 4.1041 - accuracy: 0.1319\n",
      "Epoch 15/100\n",
      "15462/15462 [==============================] - 18s 1ms/sample - loss: 4.0099 - accuracy: 0.1439\n",
      "Epoch 16/100\n",
      "15462/15462 [==============================] - 14s 925us/sample - loss: 3.9311 - accuracy: 0.1562\n",
      "Epoch 17/100\n",
      "15462/15462 [==============================] - 14s 927us/sample - loss: 3.8462 - accuracy: 0.1679\n",
      "Epoch 18/100\n",
      "15462/15462 [==============================] - 13s 857us/sample - loss: 3.7696 - accuracy: 0.1765\n",
      "Epoch 19/100\n",
      "15462/15462 [==============================] - 14s 885us/sample - loss: 3.6911 - accuracy: 0.1889\n",
      "Epoch 20/100\n",
      "15462/15462 [==============================] - 13s 811us/sample - loss: 3.6197 - accuracy: 0.2031\n",
      "Epoch 21/100\n",
      "15462/15462 [==============================] - 13s 816us/sample - loss: 3.5584 - accuracy: 0.2109\n",
      "Epoch 22/100\n",
      "15462/15462 [==============================] - 13s 820us/sample - loss: 3.4933 - accuracy: 0.2181\n",
      "Epoch 23/100\n",
      "15462/15462 [==============================] - 12s 807us/sample - loss: 3.4305 - accuracy: 0.2330\n",
      "Epoch 24/100\n",
      "15462/15462 [==============================] - 13s 824us/sample - loss: 3.3774 - accuracy: 0.2388\n",
      "Epoch 25/100\n",
      "15462/15462 [==============================] - 13s 820us/sample - loss: 3.3168 - accuracy: 0.2472\n",
      "Epoch 26/100\n",
      "15462/15462 [==============================] - 12s 805us/sample - loss: 3.2662 - accuracy: 0.2617\n",
      "Epoch 27/100\n",
      "15462/15462 [==============================] - 12s 808us/sample - loss: 3.2171 - accuracy: 0.2718\n",
      "Epoch 28/100\n",
      "15462/15462 [==============================] - 13s 825us/sample - loss: 3.1701 - accuracy: 0.2765\n",
      "Epoch 29/100\n",
      "15462/15462 [==============================] - 13s 821us/sample - loss: 3.1275 - accuracy: 0.2801\n",
      "Epoch 30/100\n",
      "15462/15462 [==============================] - 13s 817us/sample - loss: 3.0677 - accuracy: 0.2989\n",
      "Epoch 31/100\n",
      "15462/15462 [==============================] - 12s 799us/sample - loss: 3.0402 - accuracy: 0.2980\n",
      "Epoch 32/100\n",
      "15462/15462 [==============================] - 11s 697us/sample - loss: 2.9971 - accuracy: 0.3071\n",
      "Epoch 33/100\n",
      "15462/15462 [==============================] - 11s 687us/sample - loss: 2.9529 - accuracy: 0.3175\n",
      "Epoch 34/100\n",
      "15462/15462 [==============================] - 13s 815us/sample - loss: 2.9161 - accuracy: 0.3223\n",
      "Epoch 35/100\n",
      "15462/15462 [==============================] - 14s 924us/sample - loss: 2.8752 - accuracy: 0.3333\n",
      "Epoch 36/100\n",
      "15462/15462 [==============================] - 13s 816us/sample - loss: 2.8443 - accuracy: 0.3385\n",
      "Epoch 37/100\n",
      "15462/15462 [==============================] - 15s 964us/sample - loss: 2.8082 - accuracy: 0.3458\n",
      "Epoch 38/100\n",
      "15462/15462 [==============================] - 13s 828us/sample - loss: 2.7787 - accuracy: 0.3492\n",
      "Epoch 39/100\n",
      "15462/15462 [==============================] - 13s 815us/sample - loss: 2.7473 - accuracy: 0.3547\n",
      "Epoch 40/100\n",
      "15462/15462 [==============================] - 12s 807us/sample - loss: 2.7032 - accuracy: 0.3608\n",
      "Epoch 41/100\n",
      "15462/15462 [==============================] - 12s 797us/sample - loss: 2.6773 - accuracy: 0.3639\n",
      "Epoch 42/100\n",
      "15462/15462 [==============================] - 12s 797us/sample - loss: 2.6450 - accuracy: 0.3754\n",
      "Epoch 43/100\n",
      "15462/15462 [==============================] - 12s 793us/sample - loss: 2.6246 - accuracy: 0.3815\n",
      "Epoch 44/100\n",
      "15462/15462 [==============================] - 12s 790us/sample - loss: 2.5794 - accuracy: 0.3893\n",
      "Epoch 45/100\n",
      "15462/15462 [==============================] - 13s 823us/sample - loss: 2.5582 - accuracy: 0.3937\n",
      "Epoch 46/100\n",
      "15462/15462 [==============================] - 14s 918us/sample - loss: 2.5328 - accuracy: 0.3995\n",
      "Epoch 47/100\n",
      "15462/15462 [==============================] - 12s 806us/sample - loss: 2.4994 - accuracy: 0.4035\n",
      "Epoch 48/100\n",
      "15462/15462 [==============================] - 12s 795us/sample - loss: 2.4782 - accuracy: 0.4104\n",
      "Epoch 49/100\n",
      "15462/15462 [==============================] - 12s 797us/sample - loss: 2.4525 - accuracy: 0.4161\n",
      "Epoch 50/100\n",
      "15462/15462 [==============================] - 12s 791us/sample - loss: 2.4251 - accuracy: 0.4199\n",
      "Epoch 51/100\n",
      "15462/15462 [==============================] - 12s 801us/sample - loss: 2.4160 - accuracy: 0.4188\n",
      "Epoch 52/100\n",
      "15462/15462 [==============================] - 12s 801us/sample - loss: 2.3812 - accuracy: 0.4301\n",
      "Epoch 53/100\n",
      "15462/15462 [==============================] - 12s 792us/sample - loss: 2.3638 - accuracy: 0.4362\n",
      "Epoch 54/100\n",
      "15462/15462 [==============================] - 11s 714us/sample - loss: 2.3276 - accuracy: 0.4429\n",
      "Epoch 55/100\n",
      "15462/15462 [==============================] - 11s 685us/sample - loss: 2.3040 - accuracy: 0.4486\n",
      "Epoch 56/100\n",
      "15462/15462 [==============================] - 11s 689us/sample - loss: 2.2836 - accuracy: 0.4521\n",
      "Epoch 57/100\n",
      "15462/15462 [==============================] - 11s 690us/sample - loss: 2.2727 - accuracy: 0.4530\n",
      "Epoch 58/100\n",
      "15462/15462 [==============================] - 11s 686us/sample - loss: 2.2470 - accuracy: 0.4574\n",
      "Epoch 59/100\n",
      "15462/15462 [==============================] - 11s 682us/sample - loss: 2.2280 - accuracy: 0.4637\n",
      "Epoch 60/100\n",
      "15462/15462 [==============================] - 11s 685us/sample - loss: 2.2095 - accuracy: 0.4680\n",
      "Epoch 61/100\n",
      "15462/15462 [==============================] - 11s 684us/sample - loss: 2.1868 - accuracy: 0.4704\n",
      "Epoch 62/100\n",
      "15462/15462 [==============================] - 11s 686us/sample - loss: 2.1672 - accuracy: 0.4741\n",
      "Epoch 63/100\n",
      "15462/15462 [==============================] - 11s 686us/sample - loss: 2.1536 - accuracy: 0.4748\n",
      "Epoch 64/100\n",
      "15462/15462 [==============================] - 11s 680us/sample - loss: 2.1337 - accuracy: 0.4849\n",
      "Epoch 65/100\n",
      "15462/15462 [==============================] - 11s 684us/sample - loss: 2.1196 - accuracy: 0.4890\n",
      "Epoch 66/100\n",
      "15462/15462 [==============================] - 11s 688us/sample - loss: 2.0867 - accuracy: 0.4945\n",
      "Epoch 67/100\n",
      "15462/15462 [==============================] - 11s 695us/sample - loss: 2.0805 - accuracy: 0.4946\n",
      "Epoch 68/100\n",
      "15462/15462 [==============================] - 10s 679us/sample - loss: 2.0531 - accuracy: 0.5005\n",
      "Epoch 69/100\n",
      "15462/15462 [==============================] - 169s 11ms/sample - loss: 2.0349 - accuracy: 0.5021\n",
      "Epoch 70/100\n",
      "15462/15462 [==============================] - 17s 1ms/sample - loss: 2.0360 - accuracy: 0.5037\n",
      "Epoch 71/100\n",
      "15462/15462 [==============================] - 13s 868us/sample - loss: 2.0233 - accuracy: 0.5039\n",
      "Epoch 72/100\n",
      "15462/15462 [==============================] - 12s 803us/sample - loss: 1.9957 - accuracy: 0.5140\n",
      "Epoch 73/100\n",
      "15462/15462 [==============================] - 12s 788us/sample - loss: 1.9900 - accuracy: 0.5103\n",
      "Epoch 74/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15462/15462 [==============================] - 12s 793us/sample - loss: 1.9664 - accuracy: 0.5179\n",
      "Epoch 75/100\n",
      "15462/15462 [==============================] - 12s 790us/sample - loss: 1.9478 - accuracy: 0.5232\n",
      "Epoch 76/100\n",
      "15462/15462 [==============================] - 12s 790us/sample - loss: 1.9471 - accuracy: 0.5235\n",
      "Epoch 77/100\n",
      "15462/15462 [==============================] - 12s 802us/sample - loss: 1.9293 - accuracy: 0.5252\n",
      "Epoch 78/100\n",
      "15462/15462 [==============================] - 12s 798us/sample - loss: 1.9122 - accuracy: 0.5285\n",
      "Epoch 79/100\n",
      "15462/15462 [==============================] - 13s 814us/sample - loss: 1.9135 - accuracy: 0.5314\n",
      "Epoch 80/100\n",
      "15462/15462 [==============================] - 17s 1ms/sample - loss: 1.8956 - accuracy: 0.5363\n",
      "Epoch 81/100\n",
      "15462/15462 [==============================] - 13s 814us/sample - loss: 1.8516 - accuracy: 0.5444\n",
      "Epoch 82/100\n",
      "15462/15462 [==============================] - 13s 826us/sample - loss: 1.8574 - accuracy: 0.5413\n",
      "Epoch 83/100\n",
      "15462/15462 [==============================] - 11s 736us/sample - loss: 1.8590 - accuracy: 0.5391\n",
      "Epoch 84/100\n",
      "15462/15462 [==============================] - 12s 788us/sample - loss: 1.8272 - accuracy: 0.5459\n",
      "Epoch 85/100\n",
      "15462/15462 [==============================] - 13s 873us/sample - loss: 1.8454 - accuracy: 0.5459\n",
      "Epoch 86/100\n",
      "15462/15462 [==============================] - 13s 859us/sample - loss: 1.8212 - accuracy: 0.5484\n",
      "Epoch 87/100\n",
      "15462/15462 [==============================] - 14s 891us/sample - loss: 1.7942 - accuracy: 0.5550\n",
      "Epoch 88/100\n",
      "15462/15462 [==============================] - 13s 857us/sample - loss: 1.7861 - accuracy: 0.5589\n",
      "Epoch 89/100\n",
      "15462/15462 [==============================] - 10s 671us/sample - loss: 1.7619 - accuracy: 0.5666\n",
      "Epoch 90/100\n",
      "15462/15462 [==============================] - 11s 706us/sample - loss: 1.7658 - accuracy: 0.5634\n",
      "Epoch 91/100\n",
      "15462/15462 [==============================] - 11s 702us/sample - loss: 1.7647 - accuracy: 0.5626\n",
      "Epoch 92/100\n",
      "15462/15462 [==============================] - 11s 716us/sample - loss: 1.7476 - accuracy: 0.5621\n",
      "Epoch 93/100\n",
      "15462/15462 [==============================] - 11s 715us/sample - loss: 1.7391 - accuracy: 0.5687\n",
      "Epoch 94/100\n",
      "15462/15462 [==============================] - 11s 710us/sample - loss: 1.7296 - accuracy: 0.5716\n",
      "Epoch 95/100\n",
      "15462/15462 [==============================] - 11s 691us/sample - loss: 1.7181 - accuracy: 0.5716\n",
      "Epoch 96/100\n",
      "15462/15462 [==============================] - 11s 723us/sample - loss: 1.6937 - accuracy: 0.5785\n",
      "Epoch 97/100\n",
      "15462/15462 [==============================] - 12s 791us/sample - loss: 1.7038 - accuracy: 0.5751\n",
      "Epoch 98/100\n",
      "15462/15462 [==============================] - 12s 785us/sample - loss: 1.6783 - accuracy: 0.5823\n",
      "Epoch 99/100\n",
      "15462/15462 [==============================] - 13s 813us/sample - loss: 1.6759 - accuracy: 0.5802\n",
      "Epoch 100/100\n",
      "15462/15462 [==============================] - 12s 802us/sample - loss: 1.6527 - accuracy: 0.5885\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x676101a50>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(predictions,label,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That thereby beauty's rose might never die be shown find room hang clears night back still ' ' your might bring all live thee so not so thou taste not word doth true stand on your glory alone green me thee store ' room room how there of word lies right you thy clock for thine eye by me ever thee alone thee you know so eyes me due of thee thy sorrows parts decay show thee thee room room room room room room room room none of heart thy wit had dumb sweetness set it dead ' you are hate lies none none bright praise doth\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"That thereby beauty's rose might never die\"\n",
    "next_words = 100\n",
    "  \n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_len-1, padding='pre')\n",
    "    predicted = model.predict_classes(token_list, verbose=0)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_text += \" \" + output_word\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
